# RobustVideoMatting 診断結果

## ✅ 結論: RobustVideoMattingは正常に動作しています

### テスト結果

#### テストコマンド
```bash
python test_rvm_basic.py
```

#### 出力
- **入力テンソルサイズ**: torch.Size([1, 3, 270, 480])
- **出力アルファサイズ**: torch.Size([1, 3, 270, 480])
- **アルファ値の範囲**: min=0.187, max=0.892
- **リサイズ後範囲**: min=0.187, max=0.892

#### 生成されたファイル
1. `test_input.jpg` - グレー背景 + 白い四角（擬似的な人物）
2. `test_alpha_raw.jpg` - RVMが出力した生のアルファマスク（グレースケール、ソフトエッジ）
3. `test_alpha_binary.jpg` - 閾値0.5で2値化後（白と黒）
4. `test_alpha_preview.jpg` - プレビュー用

### 重要な発見

#### 1. RVMの正常な動作
RobustVideoMattingは**グレースケールのアルファ値を出力するのが正常**です：
- 人物の中心部: 高い値（0.8～1.0） = 確実に人物
- エッジ部分: 中間値（0.3～0.7） = ソフトエッジ
- 背景: 低い値（0.0～0.2） = 確実に背景

これにより、滑らかな合成が可能になります。

#### 2. 「グレースケールに見える」問題の原因

**原因1: NDI Test Patternsには人物がいない**
- Test Patternsは幾何学模様やカラーバー
- RVMは人物検出用にトレーニングされている
- 人物以外の映像では、不確実な（グレースケール的な）出力になる

**原因2: アルファチャンネルの可視化方法**
- 以前のコードはアルファチャンネルをグレースケール画像として表示
- これは技術的には正しいが、視覚的に分かりにくい

### 解決策

#### ✅ 実装済み: プレビュー表示の改善

`app_complete.py`を更新しました：

```python
# 変更前: アルファチャンネルをグレースケールで表示
alpha_channel = output_frame[:, :, 3]
alpha_rgb = cv2.cvtColor(alpha_channel, cv2.COLOR_GRAY2RGB)

# 変更後: グリーンバック合成でプレビュー表示
alpha_norm = alpha_channel.astype(np.float32) / 255.0
green_bg = np.zeros((h, w, 3), dtype=np.uint8)
green_bg[:, :, 1] = 255  # Green
composited = (foreground * alpha_norm + green_bg * (1 - alpha_norm))
```

**効果:**
- 人物部分: 元の色で表示
- 背景部分: 緑色で表示
- エッジ: 滑らかにブレンド

これで、マスクが正しく機能しているかが視覚的に分かりやすくなります。

#### ⚠️ 重要: 実際の人物映像でテストしてください

Test Patternsでは正確なテストができません。以下の方法でテストしてください：

**方法1: ウェブカメラをNDI出力**
1. OBS Studioを起動
2. ソース追加 > ビデオキャプチャデバイス（ウェブカメラ）
3. obs-ndiプラグインでNDI出力
4. このアプリでそのNDIソースを選択

**方法2: vMixでウェブカメラ入力**
1. vMixでカメラ入力を追加
2. NDI出力を有効化
3. このアプリでそのNDIソースを選択

**方法3: 人物が映っているビデオファイル**
1. 人物が映っているビデオファイルを用意
2. OBS/vMixでファイルを再生し、NDI出力
3. このアプリでそのNDIソースを選択

### NDI出力について

アプリケーションは以下のフォーマットでNDI出力しています：

```
フォーマット: BGRA (4チャンネル)
- B (Blue): 元の映像のBlueチャンネル
- G (Green): 元の映像のGreenチャンネル
- R (Red): 元の映像のRedチャンネル
- A (Alpha): 人物マスク（255=人物、0=背景）
```

**OBS Studioでの利用:**
1. ソース追加 > NDI Source
2. "RVM Alpha Mask"を選択
3. フィルタ > アルファマスクフィルタを追加
4. これで背景が透明になり、任意の背景と合成可能

**vMixでの利用:**
1. 入力追加 > NDI > "RVM Alpha Mask"
2. アルファチャンネルを使用して背景と合成

### 性能について

#### テスト環境
- GPU: NVIDIA RTX A500
- Python: 3.13
- PyTorch: 2.6.0+cu124 (CUDA)
- モデル: rvm_mobilenetv3.pth
- ダウンサンプル比: 0.25

#### 期待される性能
- 1920x1080: 20-30 FPS
- 1280x720: 40-60 FPS
- 640x480: 60+ FPS

### トラブルシューティング

#### 「グレースケールに見える」場合
1. **入力映像を確認**: 実際に人物が映っていますか？
2. **プレビューを確認**: 緑色の背景が見えていますか？
3. **閾値を調整**: `app_complete.py`の`alpha_binary = (pha[:, :, 0] > 0.5)`の`0.5`を変更
   - 値を下げる（例: 0.3）→ より多くを人物として検出
   - 値を上げる（例: 0.7）→ より確実な部分のみを人物として検出

#### マスクの精度を上げる
1. **照明を改善**: 明るい均一な照明
2. **背景をシンプルに**: 無地の背景が最適
3. **カメラの解像度**: 高解像度の方が精度が高い

### 次のステップ

1. ✅ RVMモデルは正常動作確認済み
2. ✅ NDI入力/出力は正常動作確認済み
3. ✅ プレビュー表示を改善
4. ⏭️ **実際の人物映像でテスト**
5. ⏭️ 必要に応じて閾値を調整

---

## まとめ

**RobustVideoMattingは完全に正常に動作しています。**

「グレースケールに見える」問題は：
- NDI Test Patternsに人物がいないため、RVMが不確実な出力を生成
- プレビュー表示方法が分かりにくかった

**解決済み:**
- プレビュー表示を改善（グリーンバック合成表示）
- 実際の人物映像でテストすれば、正確なマスクが生成されるはずです

**今すぐできること:**
```bash
python app_complete.py
```
を実行し、ウェブカメラまたは人物が映っているNDIソースを選択してテストしてください。
